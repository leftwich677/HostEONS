# GPT-4o 使用指南：免费体验 GPT-4 级别的多模态 AI

## 简介
OpenAI 在 2024 年春季推出了 GPT-4o，这是其最新的旗舰模型，能够实时处理音频、视觉和文本数据并进行推理。这款模型标志着人机交互的自然化进程迈出了重要一步。

## 模型能力
GPT-4o（“o”代表“omni”）接受文本、音频和图像的任意组合作为输入，并生成文本、音频和图像的任意组合输出。它可以在短至 232 毫秒的时间内响应音频输入，平均为 320 毫秒，与人类的响应时间相似。

## 性能提升
GPT-4o 在英语文本和代码上的性能与 GPT-4 Turbo 相当，在非英语文本上的性能显著提高，同时 API 的速度也更快，成本降低了 50%。与现有模型相比，GPT-4o 在视觉和音频理解方面尤为出色。

## 模型评估
### 文本评价
GPT-4o 在 0-shot COT MMLU（常识问题）上创下了 88.7% 的新高分。此外，在传统的 5-shot no-CoT MMLU 上，GPT-4o 创下了 87.2% 的新高分。

### 音频 ASR 性能
GPT-4o 比 Whisper-v3 显著提高了所有语言的语音识别性能，特别是对于资源匮乏的语言。

### 音频翻译性能
GPT-4o 在语音翻译方面树立了新的最先进水平，并且在 MLS 基准测试中优于 Whisper-v3。

### M3Exam
M3Exam 基准测试既是多语言评估也是视觉评估，由来自其他国家标准化测试的多项选择题组成，有时还包括图形和图表。在所有语言的基准测试中，GPT-4o 都比 GPT-4 更强。

### 视觉理解评估
GPT-4o 在视觉感知基准上实现了最先进的性能。所有视觉评估都是 0-shot，其中 MMMU、MathVista 和 ChartQA 作为 0-shot CoT。

## 可用性
GPT-4o 的文本和图像功能今天开始在 ChatGPT 中推出。OpenAI 正在免费套餐中提供 GPT-4o，并向 Plus 用户提供高达 5 倍的消息限制。OpenAI 将在未来几周内在 ChatGPT Plus 中推出新版本的语音模式 GPT-4o alpha。

开发人员现在还可以在 API 中访问 GPT-4o 作为文本和视觉模型。与 GPT-4 Turbo 相比，GPT-4o 速度提高 2 倍，价格降低一半，速率限制提高 5 倍。OpenAI 计划在未来几周内在 API 中向一小部分值得信赖的合作伙伴推出对 GPT-4o 新音频和视频功能的支持。

## ChatGPT 免费用户功能
OpenAI 官博还介绍了，ChatGPT 免费用户可以访问新模型加持下的功能，包括：
- 体验 GPT-4 级别的智能
- 从联网后的模型得到响应
- 分析数据并创建图表
- 畅聊你拍的照片
- 上传文件以帮助总结、撰写或分析
- 发现和使用 GPTs 和 GPT Store
- 用记忆构建更有用的体验

## 免费向所有人提供 GPT-4 级别的 AI
这款全新的 AI 模型，免费向所有人提供 GPT-4 级别的 AI。现在，进入 ChatGPT 页面，Plus 用户可以抢先体验「最新、最先进的模型」GPT-4o。

👉 [WildCard | 一分钟注册，轻松订阅海外线上服务](https://bbtdd.com/WildCard)

## 总结
GPT-4o 的最大意义在于，将 GPT-4 级别的智能带给了 OpenAI 的每一位用户！无论你是付费用户，还是免费用户，都能通过它体验 GPT-4 了。唯一不同的是，ChatGPT Plus 的消息限制是免费用户的 5 倍。

GPT-4o 不仅提供与 GPT-4 同等程度的模型能力，推理速度还更快，还能提供同时理解文本、图像、音频等内容的多模态能力。这场发布会证明了 OpenAI 在 AI 领域的领导地位。